{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 661: Homework #1\n",
    "### Linear Model, Back Propagation and Building a CNN\n",
    "\n",
    "_Name: `Suim Park(NetID:sp699)`_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 True/False Questions (10 pts)\n",
    "For each question, please provide a short explanation to support your judgment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem 1.1 (2 pts)__ On image recognition tasks, the convolution layers, compared to fully-connected layers, usually lead to better performance by exploiting shift invariant images features and having more parameters.\n",
    "> __False__; Convolutional layers lead to better performance in image recognition tasks by efficiently capturing local features and being shift-invariant, all while using fewer parameters than fully-connected layers due to parameter sharing.\n",
    "\n",
    "__Problem 1.2 (2 pts)__ According to the “convolution shape rule,” for a convolution operation with a fixed input feature map, increasing the height and width of kernel size can not lead to the output feature maps in same size.\n",
    "> __False__; According to the “convolution shape rule,” the size of the output feature map depends on several factors, including the input size, kernel size, stride, and padding. If you increase the height and width of the kernel size while keeping the input size, stride, and padding constant, the output feature map will shrink because the convolution operation reduces the spatial dimensions.\n",
    "\n",
    "__Problem 1.3 (2 pts)__ The overfitting models can perfectly fit the training data. Theoretically, we should increase slightly the noise in the training data or prune some of the nodes to improve NN’s generalization ability.\n",
    "> __True__; When a model is overfitted, it fits the training data perfectly but performs poorly on new data due to a lack of generalization. To improve generalization, slightly increasing noise in the training data can help the model focus on broader patterns rather than memorizing specific details. Pruning, or removing unnecessary neurons or connections in the neural network, is another effective method to reduce model complexity and prevent it from learning irrelevant patterns, thus improving its performance on unseen data. Both approaches enhance the model’s ability to generalize.\n",
    "\n",
    "__Problem 1.4 (2 pts)__ The latency of a neural network measured on a specific processor is not always positively related to its theoretical FLOPS.\n",
    "> __True__; The latency of a neural network is not always positively related to its theoretical FLOPS. While FLOPS measures a processor’s computational capacity, factors like memory bandwidth, data access patterns, parallelism, processor architecture, and communication overheads can affect the actual latency. Even if a processor has high FLOPS, inefficiencies in these areas can cause higher latency, meaning the two are not directly correlated.\n",
    "\n",
    "__Problem 1.5 (2 pts)__ Given a learning task that can be perfectly learned by a Madaline model, this model is suitable for different weight initializations.\n",
    "> __False__; The Madaline model has complexity due to its structure and training method, which can make it difficult to scale to large models. In addition, the Madaline model is sensitive to the random selection of training patterns and the initial weight values, making it challenging to determine when to stop training. These factors can lead to inconsistent results depending on the initialization, which is why it is not always suitable for different weight initializations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Adalines (15 pts)\n",
    "In the following problems, you will be asked to derive the output of a given Adaline, or propose proper weight values for the Adaline to mimic the functionality of some simple logic functions. For all problems, please consider +1 as __True__ and −1 as __False__ in the inputs and outputs. __The answer of the proposed weight values should be within this set {-1, 0, 1}.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem 2.1 (3 pts)__ Observe the Adaline shown in Figure 1, fill in the feature _s_ and output _y_ for each pair of inputs given in the truth table. What logic function is this Adaline performing?\n",
    "| *x<sub>1</sub>*   | *x<sub>2</sub>*   | *s*   | *y*   |\n",
    "|------------|------------|------------|------------|\n",
    "| -1 | -1 | (-1)x2 + (-1)x1 + 1x2 = __-1__ | __-1__ |\n",
    "| -1 | +1 | (-1)x2 + 1x2 = __0__ | __+1__ |\n",
    "| +1 | -1 | 1x2 + (-1)x2 = __0__ | __+1__ |\n",
    "| +1 | +1 | 1x2 + 1x2 = __4__ | __+1__ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This Adaline is performing the __`OR`__ function. It outputs __-1__ only when both input values are __-1__. However, if one or both of the inputs is __+1__, the output value is __+1__, which aligns with the behavior of the OR function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem 2.2 (4 pts)__ Propose proper values for weight *w<sub>0</sub>*, *w<sub>1</sub>* and *w<sub>2</sub>* in the Adaline shown in Figure 2 to perform the functionality of a logic __NAND__ function. Fill in the feature *s* for each pair of inputs given in the truth table to prove the functionality is correct. [__Hint__: The truth table of NAND function can be found here. https://en.wikipedia.org/wiki/NAND_logic]\n",
    "| *x<sub>1</sub>*   | *x<sub>2</sub>*   | *s*   | *y*   |\n",
    "|------------|------------|------------|------------|\n",
    "| -1 | -1 |  | +1 |\n",
    "| -1 | +1 |  | +1 |\n",
    "| +1 | -1 |  | +1 |\n",
    "| +1 | +1 |  | -1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. *w<sub>0</sub>* - *w<sub>1</sub>* - *w<sub>2</sub>* > 0\n",
    "> 2. *w<sub>0</sub>* - *w<sub>1</sub>* + *w<sub>2</sub>* > 0\n",
    "> 3. *w<sub>0</sub>* + *w<sub>1</sub>* - *w<sub>2</sub>* > 0\n",
    "> 4. *w<sub>0</sub>* + *w<sub>1</sub>* + *w<sub>2</sub>* < 0\n",
    "\n",
    "> Based on the 4 inequalities, *w<sub>0</sub>* > *w<sub>1</sub>*, *w<sub>0</sub>* > *w<sub>2</sub>* and *w<sub>0</sub>* > 0. Therefore, __*w<sub>0</sub>* = 1__ and *w<sub>1</sub>* = 0 or -1 and *w<sub>2</sub>* = 0 or -1.</br>\n",
    "> After applying various values to find the appropriate *<sub>y</sub>* for each input, the resulting weights are __*w<sub>0</sub>* = 1__, __*w<sub>1</sub>* = -1__, and __*w<sub>2</sub>* = -1__.\n",
    "\n",
    "| *x<sub>1</sub>*   | *x<sub>2</sub>*   | *s*   | *y*   |\n",
    "|------------|------------|------------|------------|\n",
    "| -1 | -1 | 1x1 + (-1)x(-1) + (-1)x(-1) = __3__ | +1 |\n",
    "| -1 | +1 | 1x1 + (-1)x(-1) + (-1)x1 = __1__ | +1 |\n",
    "| +1 | -1 | 1x1 + (-1)x1 + (-1)x(-1) = __1__ | +1 |\n",
    "| +1 | +1 | 1x1 + (-1)x1 + (-1)x1 = __-1__ | -1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem 2.3 (4 pts)__ Propose proper values for weight *w<sub>0</sub>*, *w<sub>1</sub>* and *w<sub>2</sub>* in the Adaline shown in Figure 3 to perform the functionality of a __Majority Vote__ function. Fill in the feature *s* for each triplet of inputs given in the truth table to prove the functionality is correct. [__Hint__: The truth table of Majority Vote function can be found here. https://en.wikichip.org/wiki/boolean_algebra/majority_function]\n",
    "| *x<sub>1</sub>*  | *x<sub>2</sub>*   | *x<sub>3</sub>*   | *s*   | *y*   |\n",
    "|------------|------------|------------|------------|------------|\n",
    "| -1 | -1 | -1|  | -1 |\n",
    "| -1 | -1 | +1|  | -1 |\n",
    "| -1 | +1 | -1|  | -1 |\n",
    "| -1 | +1 | +1|  | +1 |\n",
    "| +1 | -1 | -1|  | -1 |\n",
    "| +1 | -1 | +1|  | +1 |\n",
    "| +1 | +1 | -1|  | +1 |\n",
    "| +1 | +1 | +1|  | +1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. *w<sub>0</sub>* - *w<sub>1</sub>* - *w<sub>2</sub>* - *w<sub>3</sub>* < 0\n",
    "> 2. *w<sub>0</sub>* - *w<sub>1</sub>* - *w<sub>2</sub>* + *w<sub>3</sub>* < 0\n",
    "> 3. *w<sub>0</sub>* - *w<sub>1</sub>* + *w<sub>2</sub>* - *w<sub>3</sub>* < 0\n",
    "> 4. *w<sub>0</sub>* - *w<sub>1</sub>* + *w<sub>2</sub>* + *w<sub>3</sub>* > 0\n",
    "> 5. *w<sub>0</sub>* + *w<sub>1</sub>* - *w<sub>2</sub>* - *w<sub>3</sub>* < 0\n",
    "> 6. *w<sub>0</sub>* + *w<sub>1</sub>* - *w<sub>2</sub>* + *w<sub>3</sub>* > 0\n",
    "> 7. *w<sub>0</sub>* + *w<sub>1</sub>* + *w<sub>2</sub>* - *w<sub>3</sub>* > 0\n",
    "> 8. *w<sub>0</sub>* + *w<sub>1</sub>* + *w<sub>2</sub>* + *w<sub>3</sub>* > 0\n",
    "\n",
    "> Based on the 8 inequalities, they can be organized as follows:\n",
    "> 1. *w<sub>0</sub>* < *w<sub>1</sub>*, *w<sub>0</sub>* < *w<sub>2</sub>*, *w<sub>0</sub>* < *w<sub>1</sub>* + *w<sub>2</sub>*\n",
    "> 2. *w<sub>0</sub>* > -*w<sub>2</sub>*, *w<sub>0</sub>* > -*w<sub>3</sub>*, *w<sub>0</sub>* > -(*w<sub>2</sub>* + *w<sub>3</sub>*)\n",
    "\n",
    "> Using the organized inequalities above, the following result was obtained: $w_0 = 0$, $w_1 = 1$, $w_2 = 1$, $w_3 = 1$.\n",
    "> | $x_1$  | $x_2$   | $x_3$   | $s$   | $y$   |\n",
    "> |------------|------------|------------|------------|------------|\n",
    "> | -1 | -1 | -1| 0x1 + 1x(-1) + 1x(-1) + 1x(-1) = __-3__ | -1 |\n",
    "> | -1 | -1 | +1| 0x1 + 1x(-1) + 1x(-1) + 1x1 = __-1__ | -1 |\n",
    "> | -1 | +1 | -1| 0x1 + 1x(-1) + 1x1 + 1x(-1) = __-1__ | -1 |\n",
    "> | -1 | +1 | +1| 0x1 + 1x(-1) + 1x1 + 1x1 = __1__ | +1 |\n",
    "> | +1 | -1 | -1| 0x1 + 1x1 + 1x(-1) + 1x(-1) = __-1__ | -1 |\n",
    "> | +1 | -1 | +1| 0x1 + 1x1 + 1x(-1) + 1x1 = __1__ | +1 |\n",
    "> | +1 | +1 | -1| 0x1 + 1x1 + 1x1 + 1x(-1) = __1__ | +1 |\n",
    "> | +1 | +1 | +1| 0x1 + 1x1 + 1x1 + 1x1 = __3__ | +1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem 2.4 (4 pts)__ As discussed in Lecture 2, the XOR function cannot be represented with a single Adaline, but can be represented with a 2-layer Madaline. Propose proper values for second-layer weight $w_{20}$, $w_{21}$ and $w_{22}$ in the Madaline shown in Figure 4 to perform the functionality of a __XOR__ function. Fill in the feature *s* for each pair of inputs given in the truth table to prove the functionality is correct.\n",
    "| $x_1$   | $x_2$   | $s$   | $y$   |\n",
    "|------------|------------|------------|------------|\n",
    "| -1 | -1 |  | __-1__ |\n",
    "| -1 | +1 |  | __+1__ |\n",
    "| +1 | -1 |  | __+1__ |\n",
    "| +1 | +1 |  | __-1__ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following table shows the first operation involving $x_1$ and $x_2$ in this context:\n",
    "> | $x_1$   | $x_2$   | $s_1$   | $y_1$   |\n",
    "> |------------|------------|------------|------------|\n",
    "> | -1 | -1 | (0.5)x1 + (-1)x(-1) + 1x(-1) = 0.5 | __+1__ |\n",
    "> | -1 | +1 | (0.5)x1 + (-1)x(-1) + 1x1 = 2.5 | __+1__ |\n",
    "> | +1 | -1 | (0.5)x1 + (-1)x1 + 1x(-1) = -1.5 | __-1__ |\n",
    "> | +1 | +1 | (0.5)x1 + (-1)x1 + 1x1 = 0.5 | __+1__ |\n",
    "</br>\n",
    "\n",
    "> The following table shows the second operation involving $x_1$ and $x_2$ in this context:\n",
    "> | $x_1$   | $x_2$   | $s_2$   | $y_2$   |\n",
    "> |------------|------------|------------|------------|\n",
    "> | -1 | -1 | (0.5)x1 + 1x(-1) + (-1)x(-1) = 0.5 | __+1__ |\n",
    "> | -1 | +1 | (0.5)x1 + 1x(-1) + (-1)x1 = -1.5 | __-1__ |\n",
    "> | +1 | -1 | (0.5)x1 + 1x1 + (-1)x(-1) = 2.5 | __+1__ |\n",
    "> | +1 | +1 | (0.5)x1 + 1x1 + (-1)x1 = 0.5 | __+1__ |\n",
    "</br>\n",
    "\n",
    "> The final operation involving two operations' results can be shown in this table:\n",
    "> | $x_1$   | $x_2$   | $x_{21}$   | $x_{22}$   | $s$   | $y$   |\n",
    "> |------------|------------|------------|------------|------------|------------|\n",
    "> | -1 | -1 | +1 | +1 |  | -1 |\n",
    "> | -1 | +1 | +1 | -1 |  | +1 |\n",
    "> | +1 | -1 | -1 | +1 |  | +1 |\n",
    "> | +1 | +1 | +1 | +1 |  | -1 |\n",
    "</br>\n",
    "\n",
    "> Based on the table, they can be organized as follows:\n",
    "> 1. $w_{20}$ + $w_{21}$ + $w_{22}$ < 0\n",
    "> 2. $w_{20}$ + $w_{21}$ - $w_{22}$ > 0\n",
    "> 3. $w_{20}$ - $w_{21}$ + $w_{22}$ > 0\n",
    "> 4. $w_{20}$ + $w_{21}$ + $w_{22}$ < 0\n",
    "\n",
    "> The inequalities above can be summarized as follows:\n",
    "> $w_{20}$ < -$w_{21}$ - $w_{22}$ and $w_{20}$ > 0\n",
    "\n",
    "> Therefore, the values of each weight are as follows: **$w_{20}$ = 1**, **$w_{21}$ = -1** and **$w_{22}$ = -1**\n",
    "> | $x_1$   | $x_2$   | $x_{21}$   | $x_{22}$   | $s$   | $y$   |\n",
    "> |------------|------------|------------|------------|------------|------------|\n",
    "> | -1 | -1 | +1 | +1 | 1x1 + (-1)x1 + (-1)x1 = __-1__ | -1 |\n",
    "> | -1 | +1 | +1 | -1 | 1x1 + (-1)x1 + (-1)x(-1) = __+1__ | +1 |\n",
    "> | +1 | -1 | -1 | +1 | 1x1 + (-1)x(-1) + (-1)x1 = __+1__ | +1 |\n",
    "> | +1 | +1 | +1 | +1 | 1x1 + (-1)x1 + (-1)x1 = __-1__ | -1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Back Propagation (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem 3.1 (10 pts)__ Consider a 2-layer fully-connected NN, where we have input $x_1 \\in \\mathbb{R}^{n \\times 1}$, hidden feature $x_2 \\in \\mathbb{R}^{m \\times 1}$, output $x_3 \\in \\mathbb{R}^{k \\times 1}$ and weights and bias $W_1 \\in \\mathbb{R}^{m \\times n}$, $W_2 \\in \\mathbb{R}^{k \\times m}$, $b_1 \\in \\mathbb{R}^{m \\times 1}$, $b_2 \\in \\mathbb{R}^{k \\times 1}$ of the two layers. The hidden features and outputs are computed as follows:\n",
    "\n",
    "$$\n",
    "x_2 = \\text{Sigmoid}(W_1 x_1 + b_1) \\tag{1} \\quad (1)\n",
    "$$\n",
    "$$\n",
    "x_3 = W_2 x_2 + b_2 \\tag{2} \\quad (2)\n",
    "$$\n",
    "\n",
    "A MSE loss function $L = \\frac{1}{2} (t - x_3)^T (t - x_3)$ is applied in the end, where $t \\in \\mathbb{R}^{k \\times 1}$ is the target value. Following the chain rule, derive the gradient $\\frac{\\partial L}{\\partial W_1}$, $\\frac{\\partial L}{\\partial W_2}$, $\\frac{\\partial L}{\\partial b_1}$, $\\frac{\\partial L}{\\partial b_2}$ in a __vectorized format__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To get a value $\\frac{\\partial L}{\\partial W_1}$, this chain rule will be used: $\\frac{\\partial L}{\\partial W_1}$ = $\\frac{\\partial L}{\\partial x_3}$ · $\\frac{\\partial x_3}{\\partial x_2}$ · $\\frac{\\partial x_2}{\\partial W_1}$ </br>\n",
    "> - $\\frac{\\partial L}{\\partial x_3} = -(t-x_3) \\quad \\in \\mathbb{R}^{k \\times 1}$\n",
    "> - $\\frac{\\partial x_3}{\\partial x_2} = W_2 = W_2^T \\quad \\in \\mathbb{R}^{m \\times k} $ \n",
    "> - $\\frac{\\partial x_2}{\\partial W_1} = (x_2 \\odot (1 - x_2))x_1 = (x_2 \\odot (1 - x_2))x_1^T \\quad \\in \\mathbb{R}^{1 \\times n}$ ($\\odot$ is element-wise multiplication; _scalar value_)</br></br>\n",
    "> Therefore, $\\frac{\\partial L}{\\partial W_1}$ is\n",
    "> $$\n",
    "> \\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial x_2} \\cdot \\frac{\\partial x_2}{\\partial W_1} = \\frac{\\partial x_3}{\\partial x_2} \\cdot \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_2}{\\partial W_1} = W_2^T \\cdot [-(t-x_3)] \\cdot [(x_2 \\odot (1 - x_2))x_1^T] = \\left[ W_2^T (-(t - x_3)) \\odot (x_2 \\odot (1 - x_2)) \\right] x_1^T \\quad \\in \\mathbb{R}^{m \\times n}\n",
    "> $$\n",
    "\n",
    "> To get a value $\\frac{\\partial L}{\\partial W_2}$, this chain rule will be used: $\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial W_2}$</br>\n",
    "> - $\\frac{\\partial L}{\\partial x_3} = -(t-x_3) \\quad \\in \\mathbb{R}^{k \\times 1}$\n",
    "> - $\\frac{\\partial x_3}{\\partial W_2} = x_2 = x_2^T \\quad \\in \\mathbb{R}^{1 \\times m} $ </br></br>\n",
    "> Therefore, $\\frac{\\partial L}{\\partial W_2}$ is\n",
    "> $$\n",
    "> \\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial W_2} = -(t-x_3) \\cdot x_2^T \\quad \\in \\mathbb{R}^{k \\times m}\n",
    "> $$\n",
    "\n",
    "> To get a value $\\frac{\\partial L}{\\partial b_1}$, this chain rule will be used: $\\frac{\\partial L}{\\partial b_1}$ = $\\frac{\\partial L}{\\partial x_3}$ · $\\frac{\\partial x_3}{\\partial x_2}$ · $\\frac{\\partial x_2}{\\partial b_1}$ </br>\n",
    "> - $\\frac{\\partial L}{\\partial x_3} = -(t-x_3) \\quad \\in \\mathbb{R}^{k \\times 1}$\n",
    "> - $\\frac{\\partial x_3}{\\partial x_2} = W_2 = W_2^T \\quad \\in \\mathbb{R}^{m \\times k} $ \n",
    "> - $\\frac{\\partial x_2}{\\partial b_1} = (x_2 \\odot (1 - x_2)) \\quad $ ($\\odot$ is element-wise multiplication; _scalar value_) </br></br>\n",
    "> Therefore, $\\frac{\\partial L}{\\partial W_1}$ is\n",
    "> $$\n",
    "> \\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial x_2} \\cdot \\frac{\\partial x_2}{\\partial b_1} = \\frac{\\partial x_3}{\\partial x_2} \\cdot \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_2}{\\partial b_1} = W_2^T \\cdot [-(t-x_3)] \\cdot (x_2 \\odot (1 - x_2)) = \\left[ W_2^T (-(t - x_3)) \\odot (x_2 \\odot (1 - x_2)) \\right]  \\quad \\in \\mathbb{R}^{m \\times 1}\n",
    "> $$\n",
    "\n",
    "> To get a value $\\frac{\\partial L}{\\partial b_2}$, this chain rule will be used: $\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial b_2}$</br>\n",
    "> - $\\frac{\\partial L}{\\partial x_3} = -(t-x_3) \\quad \\in \\mathbb{R}^{k \\times 1}$\n",
    "> - $\\frac{\\partial x_3}{\\partial b_2} = 1 \\quad \\in \\mathbb{I} $ </br></br>\n",
    "> Therefore, $\\frac{\\partial L}{\\partial b_2}$ is\n",
    "> $$\n",
    "> \\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial b_2} = -(t-x_3) \\cdot \\mathbb{I} \\quad \\in \\mathbb{R}^{k \\times 1}\n",
    "> $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem 3.2 (5 pts)__ Replace the Sigmoid function with ReLU function. Given a data $x_1 = [1, 1, −1]^T$, target value $t = [1, 2]^T$, weights and bias at this iteration are\n",
    "$$\n",
    "W_1=\\begin{bmatrix}\n",
    "0 & -1 & 1 \\\\\n",
    "2 & 2 & -1\n",
    "\\end{bmatrix}, b_1=\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\quad (3)\n",
    "$$\n",
    "$$\n",
    "W_2 = \\begin{bmatrix}\n",
    "0 & 2 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix} , b_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\quad (4)\n",
    "$$\n",
    "Following the results in Problem 3.1, calculate the values of: $L$, $\\frac{\\partial L}{\\partial W_1}$, $\\frac{\\partial L}{\\partial W_2}$, $\\frac{\\partial L}{\\partial b_1}$, $\\frac{\\partial L}{\\partial b_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using its value, $x_2$ and $x_3$ is this:\n",
    "$$\n",
    "x_2 = \\text{ReLU}(W_1 x_1 + b_1) = ReLU(\\begin{bmatrix} 0 & -1 & 1 \\\\ 2 & 2 & -1\\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 1 \\\\ -1\\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}) = ReLU(\\begin{bmatrix} -2 \\\\ 5 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}) = ReLU(\\begin{bmatrix} -1 \\\\ 7 \\end{bmatrix}) = \\begin{bmatrix} 0 \\\\ 7 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "x_3 = W_2 x_2 + b_2 = \\begin{bmatrix} 0 & 2 \\\\ 1 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 7 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 14 \\\\ 7 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 15 \\\\ 8 \\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To get a value $L$, this equation is used: $L = \\frac{1}{2} (t - x_3)^T (t - x_3)$ </br>\n",
    "> - $t - x_3 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 15 \\\\ 8 \\end{bmatrix} = \\begin{bmatrix} -14 \\\\ -6 \\end{bmatrix}$ </br>\n",
    "> Therefore Loss value is: $$ L = \\frac{1}{2} \\begin{bmatrix} -14 & -6 \\end{bmatrix} \\begin{bmatrix} -14 \\\\ -6 \\end{bmatrix} = \\frac{1}{2} (196 + 36) = \\frac{1}{2} \\times 232 = 116$$\n",
    "\n",
    "> To get a value $\\frac{\\partial L}{\\partial W_1}$, this chain rule will be used: $\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial x_2} \\cdot \\frac{\\partial x_2}{\\partial W_1}$ </br>\n",
    "> - $\\frac{\\partial L}{\\partial x_3}  = -(t-x_3) = - (\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}-\\begin{bmatrix} 15 \\\\ 8 \\end{bmatrix}) = \\begin{bmatrix} 14 \\\\ 6 \\end{bmatrix}$\n",
    "> - $\\frac{\\partial x_3}{\\partial x_2} = W_2 = W_2^T = \\begin{bmatrix} 0 & 1 \\\\ 2 & 1 \\end{bmatrix}$\n",
    "> - $ \\text{ReLU}{\\prime}(z) = \\begin{cases} 1 & \\text{if } z > 0 \\\\ 0 & \\text{if } z \\leq 0 \\end{cases} (z = W_1x_1 + b_1)$\n",
    "> - $\\frac{\\partial x_2}{\\partial W_1} = \\frac{\\partial x_2}{\\partial (W_1 x_1 + b_1)} \\cdot \\frac{\\partial (W_1 x_1 + b_1)}{\\partial W_1} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}x_1^T$ </br>\n",
    "> In this chain rule, $\\frac{\\partial L}{\\partial W_1}$ is this:\n",
    "> $$\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial x_2} \\cdot \\frac{\\partial x_2}{\\partial W_1} = \\left( W_2^T \\frac{\\partial L}{\\partial x_3} \\odot \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\right) x_1^T = [(\\begin{bmatrix} 0 & 1 \\\\ 2 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 14 \\\\ 6 \\end{bmatrix}) \\odot \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}]\\cdot \\begin{bmatrix} 1 & 1 & -1 \\end{bmatrix} $$\n",
    "> $$ = (\\begin{bmatrix} 6 \\\\ 34 \\end{bmatrix} \\odot \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}) \\cdot \\begin{bmatrix} 1 & 1 & -1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 34 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 1 & -1 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 & 0 \\\\ -34 & -34 & 34\\end{bmatrix}$$\n",
    "\n",
    "> To get a value $\\frac{\\partial L}{\\partial W_2}$, this chain rule will be used: $\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial W_2}$ </br>\n",
    "> - $\\frac{\\partial L}{\\partial x_3}  = -(t-x_3) = - (\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}-\\begin{bmatrix} 15 \\\\ 8 \\end{bmatrix}) = \\begin{bmatrix} 14 \\\\ 6 \\end{bmatrix}$\n",
    "> - $\\frac{\\partial x_3}{\\partial W_2} = x_2 = x_2^T = \\begin{bmatrix} 0 & 7 \\end{bmatrix}$</br>\n",
    "> In this chain rule, $\\frac{\\partial L}{\\partial W_2}$ is this:\n",
    "> $$\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial W_2} = \\frac{\\partial x_3}{\\partial W_2} \\cdot \\frac{\\partial L}{\\partial x_3} = \\begin{bmatrix} 14 \\\\ 6 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 & 7 \\end{bmatrix} = \\begin{bmatrix} 0 & 98 \\\\ 0 & 42 \\end{bmatrix}$$\n",
    "\n",
    "> To get a value $\\frac{\\partial L}{\\partial b_1}$, this chain rule will be used: $\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial x_2} \\cdot \\frac{\\partial x_2}{\\partial b_1}$ </br>\n",
    "> - $\\frac{\\partial L}{\\partial x_3}  = -(t-x_3) = - (\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}-\\begin{bmatrix} 15 \\\\ 8 \\end{bmatrix}) = \\begin{bmatrix} 14 \\\\ 6 \\end{bmatrix}$\n",
    "> - $\\frac{\\partial x_3}{\\partial x_2} = W_2 = W_2^T = \\begin{bmatrix} 0 & 1 \\\\ 2 & 1 \\end{bmatrix}$\n",
    "> - $ \\text{ReLU}{\\prime}(z) = \\begin{cases} 1 & \\text{if } z > 0 \\\\ 0 & \\text{if } z \\leq 0 \\end{cases} (z = W_1x_1 + b_1)$\n",
    "> - $\\frac{\\partial x_2}{\\partial b_1} = \\frac{\\partial x_2}{\\partial (W_1 x_1 + b_1)} \\cdot \\frac{\\partial (W_1 x_1 + b_1)}{\\partial b_1} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ </br>\n",
    "> In this chain rule, $\\frac{\\partial L}{\\partial W_1}$ is this:\n",
    "> $$\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial x_2} \\cdot \\frac{\\partial x_2}{\\partial W_1} = [W_2^T \\frac{\\partial L}{\\partial x_3}] \\odot \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = (\\begin{bmatrix} 0 & 1 \\\\ 2 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 14 \\\\ 6 \\end{bmatrix}) \\odot \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 34 \\end{bmatrix} \\odot \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 34 \\end{bmatrix}$$\n",
    "\n",
    "> To get a value $\\frac{\\partial L}{\\partial b_2}$, this chain rule will be used: $\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial b_2}$ </br>\n",
    "> - $\\frac{\\partial L}{\\partial x_3}  = -(t-x_3) = - (\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}-\\begin{bmatrix} 15 \\\\ 8 \\end{bmatrix}) = \\begin{bmatrix} 14 \\\\ 6 \\end{bmatrix}$\n",
    "> - $\\frac{\\partial x_3}{\\partial b_2} = \\mathbb{I_k}$</br>\n",
    "> In this chain rule, $\\frac{\\partial L}{\\partial W_2}$ is this:\n",
    "> $$\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial b_2} = \\begin{bmatrix} 14 \\\\ 6 \\end{bmatrix} \\cdot \\mathbb{I_k} = \\begin{bmatrix} 14 \\\\ 6 \\end{bmatrix}$$\n",
    "\n",
    "> $$\\therefore L=166, \\frac{\\partial L}{\\partial W_2} = \\begin{bmatrix} 0 & 0 & 0 \\\\ -34 & -34 & 34\\end{bmatrix}, \\frac{\\partial L}{\\partial W_2} = \\begin{bmatrix} 0 & 98 \\\\ 0 & 42 \\end{bmatrix}, \\frac{\\partial L}{\\partial W_1} = \\begin{bmatrix} 0 \\\\ 34 \\end{bmatrix}, \\frac{\\partial L}{\\partial b_2} = \\begin{bmatrix} 14 \\\\ 6 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 2D Convolution (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem 4.1 (5 pts)__ Derive the 2D convolution results of the following 5 × 9 input matrix and the 3 × 3 kernel. Consider 0s are padded around the input and the stride is 1, so that the output should also have shape 5 × 9.\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & -1 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & -1 & -1 & -1 & 0 & 1 & 1 & 1 & 0 \\\\\n",
    "-1 & -1 & -1 & -1 & 0 & 1 & 1 & 1 & 1 \\\\\n",
    "0 & -1 & -1 & -1 & 0 & 1 & 1 & 1 & 0 \\\\\n",
    "0 & 0 & -1 & 0 & 0 & 0 & 1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "0 & -1/2 & 0 \\\\\n",
    "-1/2 & 1 & -1/2 \\\\\n",
    "0 & -1/2 & 0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Input Matrix:\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0  1  0  0  0]\n",
      " [ 0  0 -1 -1 -1  0  1  1  1  0  0]\n",
      " [ 0 -1 -1 -1 -1  0  1  1  1  1  0]\n",
      " [ 0  0 -1 -1 -1  0  1  1  1  0  0]\n",
      " [ 0  0  0 -1  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]]\n",
      "\n",
      "Convolution Result (5x9):\n",
      "[[ 0.   1.  -0.5  1.   0.  -1.   0.5 -1.   0. ]\n",
      " [ 1.   0.   1.   0.   0.   0.  -1.   0.  -1. ]\n",
      " [-0.5  1.   1.   0.5  0.  -0.5 -1.  -1.   0.5]\n",
      " [ 1.   0.   1.   0.   0.   0.  -1.   0.  -1. ]\n",
      " [ 0.   1.  -0.5  1.   0.  -1.   0.5 -1.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "# input matrix\n",
    "input_matrix = np.array(\n",
    "    [\n",
    "        [0, 0, -1, 0, 0, 0, 1, 0, 0],\n",
    "        [0, -1, -1, -1, 0, 1, 1, 1, 0],\n",
    "        [-1, -1, -1, -1, 0, 1, 1, 1, 1],\n",
    "        [0, -1, -1, -1, 0, 1, 1, 1, 0],\n",
    "        [0, 0, -1, 0, 0, 0, 1, 0, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# kernel 3x3\n",
    "kernel = np.array([[0, -1 / 2, 0], [-1 / 2, 1, -1 / 2], [0, -1 / 2, 0]])\n",
    "\n",
    "# padding\n",
    "padding = 1\n",
    "padded_input_matrix = np.pad(input_matrix, padding, mode=\"constant\", constant_values=0)\n",
    "\n",
    "# convolution\n",
    "output_matrix = convolve2d(padded_input_matrix, kernel, mode=\"valid\")\n",
    "\n",
    "print(\"Padded Input Matrix:\")\n",
    "print(padded_input_matrix)\n",
    "print(\"\\nConvolution Result (5x9):\")\n",
    "print(output_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\therefore Output = \\begin{bmatrix}\n",
    "0 & 1 & -0.5 & 1 & 0 & -1 & 0.5 & -1 & 0 \\\\\n",
    "1 & 0 & 1 & 0 & 0 & 0 & -1 & 0 & -1 \\\\\n",
    "-0.5 & 1 & 1 & 0.5 & 0 & -0.5 & -1 & -1 & 0.5 \\\\\n",
    "1 & 0 & 1 & 0 & 0 & 0 & -1 & 0 & -1 \\\\\n",
    "0 & 1 & -0.5 & 1 & 0 & -1 & 0.5 & -1 & 0\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem 4.2 (5 pts)__ Compare the output matrix and the input matrix in Problem 4.1, briefly analyze the effect of this 3×3 kernel on the input. (Hint: apply this kernel to an image to see the outputs, attached the image you applied and result image)\n",
    "> Compared to the input matrix, the output matrix—resulting from the application of the 3x3 kernel—highlights detected features and enhances their characteristics. When applied to the image, this 3x3 kernel accentuates edges and sharpens features, making it useful for tasks such as edge detection and image enhancement in image processing.​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Lab: LMS Algorithm (15 pts)\n",
    "In this lab question, you will implement the LMS algorithm with NumPy to learn a linear regression model for the provided dataset. You will also be directed to analyze how the choice of learning rate in the LMS algorithm affect the final result. All the codes generating the results of this lab should be gathered in one file and submit to Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 1 (15 pts)\n",
    "To start with, please download the dataset.mat file from Canvas and load it into NumPy arrays. There are two variables in the file: data $X ∈ \\mathbb{R}^{100 \\times 3}$ and target $D ∈ \\mathbb{R}^{100 \\times 1}$. Each individual pair of data and target is composed into $X$ and $D$ following the same way as discussed in Lecture 2. Specifically, each row in $X$ correspond to the transpose of a data point, with the first element as constant 1 and the other two as the two input features $x_{1k}$ and $x_{2k}$. The goal of the learning task is finding the weight vector $W ∈ \\mathbb{R}^{3 \\times 1}$ for the linear model that can minimize the MSE loss, which is also formulated on Lecture 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) (3pt) Directly compute the least square (Wiener) solution with the provided dataset. What is the optimal weight $W^*$? What is the MSE loss of the whole dataset when the weight is set to $W^*$?\n",
    "\n",
    "> Optimal Weight: $ W^* = \\begin{bmatrix} 1.0007 \\\\ 1.0006 \\\\ -2.0003\\end{bmatrix}$, MSE Loss = $5.04e^{-05}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'D', 'X'])\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "# load the data\n",
    "file_path = \"/Users/suim/Downloads/24Fall/ECE 661 Computer Engineering Maching Learning and Deep Neural Nets/HW 1/dataset.mat\"\n",
    "data = loadmat(file_path)\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.0006781   1.00061145 -2.00031968]\n"
     ]
    }
   ],
   "source": [
    "D = data[\"D\"]\n",
    "X = data[\"X\"]\n",
    "\n",
    "# compute the least square (Wiener) solution\n",
    "w_star = np.linalg.inv(X.T @ X) @ X.T @ D\n",
    "print(w_star.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.03995157e-05]]\n"
     ]
    }
   ],
   "source": [
    "loss = (D - X @ w_star).T @ (D - X @ w_star) / 200\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) (4pt) Now consider that you can only train with 1 pair of data point and target each time. In such case, the LMS algorithm should be used to find the optimal weight. Please initialize the weight vector as $W^0 = [0, 0, 0]^T$, and update the weight with the LMS algorithm. After each $epoch$ (every time you go through all the training data and loop back to the beginning), compute and record the MSE loss of the current weight on the whole dataset. Run LMS for 20 epochs with learning rate $r = 0.005$, report the weight you get in the end and plot the MSE loss in $log scale$ vs. Epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_list = []  # save it as empty vector\n",
    "\n",
    "w = np.zeros((3, 1))\n",
    "lr = 0.005\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
